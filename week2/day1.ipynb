{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyCl\n",
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"VocÃª Ã© um assistente que Ã© Ã³timo em contar piadas\"\n",
    "user_prompt = \"Conte uma piada para um pÃºblico de cientistas de dados\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Por que os cientistas de dados nunca jogam cartas?\n",
      "\n",
      "Porque eles tÃªm medo de que a distribuiÃ§Ã£o nÃ£o seja normal! ðŸ˜„\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts, temperature=0.7)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Aqui vai uma para os cientistas de dados:\n",
      "\n",
      "Por que o cientista de dados levou um mapa para o trabalho?\n",
      "\n",
      "Porque ele nÃ£o queria perder o *data frame*! ðŸ˜„\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Por que o dado foi ao psicÃ³logo?  \n",
      "Porque ele estava se sentindo um pouco disperso e precisava de uma anÃ¡lise mais profunda!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Aqui vai uma piada para cientistas de dados:\n",
      "\n",
      "Por que o cientista de dados levou seu modelo para o bar?\n",
      "\n",
      "Porque ele queria melhorar o \"fit\" com alguns \"shots\" de tequila!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantos cientistas de dados sÃ£o necessÃ¡rios para trocar uma lÃ¢mpada?\n",
      "\n",
      "SÃ³ um. Mas antes ele precisa:\n",
      "\n",
      " 1. Definir o que significa â€œescuroâ€ (criar a mÃ©trica de luminosidade)  \n",
      " 2. Coletar um dataset de nÃ­veis de luz em diferentes cÃ´modos  \n",
      " 3. Escolher entre regressÃ£o linear, Ã¡rvore de decisÃ£o ou rede neural  \n",
      " 4. Fazer gridâ€search no nÃºmero de giros da lÃ¢mpada e no torque do parafuso  \n",
      " 5. Validar com crossâ€validation para evitar overfitting  \n",
      " 6. Ajustar hiperparÃ¢metros atÃ© a iluminaÃ§Ã£o atingir, pelo menos, 95% de acurÃ¡cia  \n",
      "\n",
      "No fim, a lÃ¢mpada ainda estÃ¡ queimadaâ€¦ enquanto o engenheiro elÃ©trico jÃ¡ trocou trÃªs!\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o4-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o4-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqui vai uma para vocÃªs:\n",
      "\n",
      "Por que os cientistas de dados nunca ficam sozinhos?\n",
      "\n",
      "Porque eles sempre tÃªm seus **clusters**! ðŸ“Š\n",
      "\n",
      "---\n",
      "\n",
      "E aqui vai um bÃ´nus:\n",
      "\n",
      "Um cientista de dados vai ao mÃ©dico e diz:\n",
      "- Doutor, estou com um problema sÃ©rio de correlaÃ§Ã£o com minha esposa.\n",
      "- CorrelaÃ§Ã£o? - pergunta o mÃ©dico.\n",
      "- Ã‰... mas nÃ£o sei se Ã© causalidade! ðŸ“ˆ\n",
      "\n",
      "*Rimshot* ðŸ¥\n"
     ]
    }
   ],
   "source": [
    "# Claude 4.0 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cafbb396-5f2e-48d6-9bb0-4270c8e45bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Aqui vai uma piada para cientistas de dados:  \n",
      "\n",
      "**Por que o cientista de dados quebrou o espelho?**  \n",
      "\n",
      "Porque ele queria trabalhar com dados *nÃ£o refletidos*!  \n",
      "\n",
      "(Se nÃ£o riu, talvez seja porque a correlaÃ§Ã£o nÃ£o implica causalidade... ou porque vocÃª jÃ¡ ouviu essa *n* vezes!) ðŸ˜„ðŸ“Š\n"
     ]
    }
   ],
   "source": [
    "response = deepseek.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Por que o cientista de dados terminou o relacionamento?\n",
      "\n",
      "Porque ele descobriu que a correlaÃ§Ã£o entre eles era estatisticamente significativa, mas quando tentou fazer uma regressÃ£o linear no relacionamento, o RÂ² era terrÃ­vel! \n",
      "\n",
      "Ele disse: \"Querida, nossos dados estÃ£o muito dispersos, tem muito ruÃ­do na nossa comunicaÃ§Ã£o, e claramente estamos overfittando. Acho que precisamos fazer um train-test split... permanente!\" ðŸ“ŠðŸ’”\n",
      "\n",
      "*BÃ´nus*: Ela respondeu que ele estava sendo muito dramÃ¡tico e que era sÃ³ fazer uma normalizaÃ§Ã£o nos dados... mas ele jÃ¡ tinha decidido que era melhor partir para um modelo nÃ£o-supervisionado! ðŸ˜„"
     ]
    }
   ],
   "source": [
    "# Claude 4.0 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# The API for Gemini has a slightly different structure.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\u001b[39;00m\n\u001b[32m      5\u001b[39m gemini = google.generativeai.GenerativeModel(\n\u001b[32m      6\u001b[39m     model_name=\u001b[33m'\u001b[39m\u001b[33mgemini-2.0-flash\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m     system_instruction=system_message\n\u001b[32m      8\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m response = \u001b[43mgemini\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:113\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, *args, **kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m     metadata.extend(\u001b[38;5;28mself\u001b[39m._metadata)\n\u001b[32m    111\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m] = metadata\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\google\\api_core\\retry.py:349\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    345\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    346\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    347\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    348\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\google\\api_core\\retry.py:191\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[32m    118\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m._timeout - time_since_first_attempt)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:65\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     67\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\grpc\\_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\grpc\\_interceptor.py:329\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interceptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:79\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     64\u001b[39m     grpc_request = {\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     68\u001b[39m     }\n\u001b[32m     69\u001b[39m     _LOGGER.debug(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         },\n\u001b[32m     77\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m response = \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     81\u001b[39m     response_metadata = response.trailing_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\grpc\\_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     new_method,\n\u001b[32m    308\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     new_compression,\n\u001b[32m    313\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\grpc\\_channel.py:1195\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1185\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1190\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1191\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m   1192\u001b[39m     (\n\u001b[32m   1193\u001b[39m         state,\n\u001b[32m   1194\u001b[39m         call,\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\grpc\\_channel.py:1162\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1145\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1146\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1147\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1161\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1163\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f0ff2-8581-4836-bf00-37fddbe120eb",
   "metadata": {},
   "source": [
    "# Sidenote:\n",
    "\n",
    "This alternative approach of using the client library from OpenAI to connect with other models has become extremely popular in recent months.\n",
    "\n",
    "So much so, that all the models now support this approach - including Anthropic.\n",
    "\n",
    "You can read more about this approach, with 4 examples, in the first section of this guide:\n",
    "\n",
    "https://github.com/ed-donner/agents/blob/main/guides/09_ai_apis_and_ollama.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "If I were to answer the question \"How many words are there in your answer to this prompt?\" with this response, the word count would be **24 words**.  \n",
       "\n",
       "Hereâ€™s the breakdown:  \n",
       "1. If  \n",
       "2. I  \n",
       "3. were  \n",
       "4. to  \n",
       "5. answer  \n",
       "6. the  \n",
       "7. question  \n",
       "8. \"How  \n",
       "9. many  \n",
       "10. words  \n",
       "11. are  \n",
       "12. there  \n",
       "13. in  \n",
       "14. your  \n",
       "15. answer  \n",
       "16. to  \n",
       "17. this  \n",
       "18. prompt?\"  \n",
       "19. with  \n",
       "20. this  \n",
       "21. response,  \n",
       "22. the  \n",
       "23. word  \n",
       "24. count  \n",
       "\n",
       "Let me know if you'd like a different example or further clarification!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 117\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, the user asked: \"How many words are there in your answer to this prompt?\" I need to respond to this question accurately.\n",
      "\n",
      "My response should include the answer to their question, which is the word count of my own response. But I have to be careful because the response itself will contain the word count, so I need to calculate it after writing the response or as part of the process.\n",
      "\n",
      "I should start by drafting my response. The response needs to state the word count, but to do that, I must know how many words are in the full response.\n",
      "\n",
      "Let me outline what my response might look like:\n",
      "\n",
      "1. Acknowledge the question.\n",
      "\n",
      "2. Provide the word count.\n",
      "\n",
      "3. Since the word count includes all words in the response, I need to count them.\n",
      "\n",
      "But the word count is about the entire answer, so I should write the response first, then count the words, and include that count in the response.\n",
      "\n",
      "This might lead to a loop: if I include the count, the count changes if I add or remove words. For example, if I say \"There are X words,\" but X is a number, it doesn't add to the word count as a word; numbers are not words unless written out.\n",
      "\n",
      "In word counting, numbers are often considered as words if they are separate entities. For instance, \"5\" might be counted as one word, or if written as \"five,\" it's one word.\n",
      "\n",
      "In this context, since I'm counting words, I should treat the number as part of the text.\n",
      "\n",
      "To avoid confusion, I should write a response that is static once completed.\n",
      "\n",
      "Let me draft a simple response:\n",
      "\n",
      "\"Hello! In my response to your prompt, there are [number] words.\"\n",
      "\n",
      "But I need to fill in the number. Once I have the full response, I can count the words.\n",
      "\n",
      "I should consider what the response will contain. For example:\n",
      "\n",
      "- Greeting: \"Hello!\" or something similar.\n",
      "\n",
      "- The answer: \"There are X words in this response.\"\n",
      "\n",
      "- Perhaps some explanation to be clear.\n",
      "\n",
      "But to minimize words, I can keep it concise.\n",
      "\n",
      "The user might expect just the number, but since it's a response, I should make it complete.\n",
      "\n",
      "Another idea: I could provide the word count for this specific sentence or something, but the question is about \"your answer to this prompt,\" which means my entire response.\n",
      "\n",
      "So, my response is what I'm typing now.\n",
      "\n",
      "I need to compose the response, count the words in it, and then state that count.\n",
      "\n",
      "But when I state the count, it affects the total word count.\n",
      "\n",
      "This is a common self-referential problem.\n",
      "\n",
      "For instance:\n",
      "\n",
      "Suppose I say: \"There are 5 words in this response.\"\n",
      "\n",
      "But let's count: \"There\" (1), \"are\" (2), \"5\" (3 â€“ if we count \"5\" as a word), \"words\" (4), \"in\" (5), \"this\" (6), \"response\" (7). That's 7 words, not 5. So it's inconsistent.\n",
      "\n",
      "If I write the number as a word: \"There are five words in this response.\" Words: \"There\" (1), \"are\" (2), \"five\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"response\" (7). Still 7 words.\n",
      "\n",
      "I need a response where the stated number matches the actual count.\n",
      "\n",
      "One way is to include the count in a way that it accounts for itself.\n",
      "\n",
      "For example: \"This response contains five words.\" But \"This response contains five words.\" has 4 words: \"This\" (1), \"response\" (2), \"contains\" (3), \"five\" (4), \"words\" (5). \"five words\" â€“ \"five\" is word 4, \"words\" is word 5. So 5 words, and I said \"five,\" which matches. Perfect.\n",
      "\n",
      "In this case: \"This response contains five words.\" â€“ that's 5 words.\n",
      "\n",
      "But is \"five\" considered a word? Yes, it's a word.\n",
      "\n",
      "Similarly, if I use a number: \"This response contains 5 words.\" â€“ \"5\" might be counted as a word or not, depending on the context. In most word counting, digits like \"5\" are counted as one word.\n",
      "\n",
      "For example, in Microsoft Word, \"5\" is counted as one word.\n",
      "\n",
      "Similarly, in text analysis, alphanumeric tokens are words.\n",
      "\n",
      "So, \"This response contains 5 words.\" â€“ words: \"This\" (1), \"response\" (2), \"contains\" (3), \"5\" (4), \"words\" (5). So 5 words, and \"5\" represents 5.\n",
      "\n",
      "But to be precise, in the statement, it says \"5 words,\" which is correct.\n",
      "\n",
      "In the first version with \"five,\" it's also correct.\n",
      "\n",
      "Now, for my actual response, I might want to be more polite or clear.\n",
      "\n",
      "The user asked: \"How many words are there in your answer to this prompt?\"\n",
      "\n",
      "So, my answer should directly address that.\n",
      "\n",
      "I can say something like: \"The number of words in my answer is 5.\" But let's count if that works.\n",
      "\n",
      "\" The number of words in my answer is 5.\" â€“ words: \"The\" (1), \"number\" (2), \"of\" (3), \"words\" (4), \"in\" (5), \"my\" (6), \"answer\" (7), \"is\" (8), \"5\" (9). That's 9 words, but I said 5, which is wrong.\n",
      "\n",
      "So that doesn't work.\n",
      "\n",
      "I need a shorter phrase.\n",
      "\n",
      "Back to the earlier idea: \"This response contains five words.\" But that might be too minimal.\n",
      "\n",
      "I can add a bit more to make it natural.\n",
      "\n",
      "For example: \"Hello! This response contains exactly five words.\"\n",
      "\n",
      "But \"Hello! This response contains exactly five words.\" â€“ let's count:\n",
      "\n",
      "\"Hello\" (1), \"This\" (2), \"response\" (3), \"contains\" (4), \"exactly\" (5), \"five\" (6), \"words\" (7). That's 7 words, but I said \"five,\" which is incorrect for the count.\n",
      "\n",
      "Problem.\n",
      "\n",
      "\"Hi. Five words here.\" But \"Hi.\" (1), \"Five\" (2), \"words\" (3), \"here\" (4). Only 4 words.\n",
      "\n",
      "Not matching.\n",
      "\n",
      "I need to construct a sentence where the word count includes the number stated.\n",
      "\n",
      "One common way is to have a statement like: \"I contain five words.\" But \"I\" (1), \"contain\" (2), \"five\" (3), \"words\" (4). Only 4.\n",
      "\n",
      "\" There are five words.\" \"There\" (1), \"are\" (2), \"five\" (3), \"words\" (4). 4 words.\n",
      "\n",
      "To get five, I could say: \"This sentence has five words.\" But \"This\" (1), \"sentence\" (2), \"has\" (3), \"five\" (4), \"words\" (5). Yes, 5 words.\n",
      "\n",
      "Perfect.\n",
      "\n",
      "But in my case, it's not a sentence, it's the response.\n",
      "\n",
      "For the response, I can make the entire response be: \"This response has five words.\"\n",
      "\n",
      "But as above, \"This response has five words.\" is 5 words.\n",
      "\n",
      "But is that sufficient for the user? It might seem abrupt.\n",
      "\n",
      "I could have a response that includes the count accurately with a bit more context.\n",
      "\n",
      "Another idea: State the word count separately or at the end.\n",
      "\n",
      "For example, I could say: \"In response to your prompt, my answer is that this text contains X words.\" But then X needs to be filled.\n",
      "\n",
      "Let's define the response.\n",
      "\n",
      "Suppose my response is: \"The word count of this answer is 10.\"\n",
      "\n",
      "But I don't know if it's 10 yet.\n",
      "\n",
      "I should write the response first, then count the words, and adjust so that the stated number matches.\n",
      "\n",
      "Since this is a thought process, I can do that.\n",
      "\n",
      "Let me draft a full response.\n",
      "\n",
      "Response draft:\n",
      "\n",
      "\"Hello! I understand your question. The number of words in my answer to this prompt is [N].\"\n",
      "\n",
      "But to find N, I need to count the words in this response.\n",
      "\n",
      "First, without the number: \"Hello! I understand your question. The number of words in my answer to this prompt is .\"\n",
      "\n",
      "But that's incomplete.\n",
      "\n",
      "Assume the number is part of the text.\n",
      "\n",
      "For counting, \"N\" could be a placeholder.\n",
      "\n",
      "Let's write the text: \"Hello! I understand your question. The number of words in my answer to this prompt is X.\"\n",
      "\n",
      "Now, count the words:\n",
      "\n",
      "- \"Hello\" : 1\n",
      "\n",
      "- \"I\" : 2\n",
      "\n",
      "- \"understand\" : 3\n",
      "\n",
      "- \"your\" : 4\n",
      "\n",
      "- \"question\" : 5\n",
      "\n",
      "- \"The\" : 6\n",
      "\n",
      "- \"number\" : 7\n",
      "\n",
      "- \"of\" : 8\n",
      "\n",
      "- \"words\" : 9\n",
      "\n",
      "- \"in\" : 10\n",
      "\n",
      "- \"my\" : 11\n",
      "\n",
      "- \"answer\" : 12\n",
      "\n",
      "- \"to\" : 13\n",
      "\n",
      "- \"this\" : 14\n",
      "\n",
      "- \"prompt\" : 15\n",
      "\n",
      "- \"is\" : 16\n",
      "\n",
      "- \"X\" : 17 (where X is the number)\n",
      "\n",
      "So there are 17 words including the placeholder for the number.\n",
      "\n",
      "But when I replace X with the actual number, say \"seventeen\" or \"17\", that is one word.\n",
      "\n",
      "So if I use \"17\", it's still 17 words: the text has 16 words before the number, plus the number makes 17.\n",
      "\n",
      "\"Hello! I understand your question. The number of words in my answer to this prompt is 17.\"\n",
      "\n",
      "Count: 1.Hello, 2.I, 3.understand, 4.your, 5.question, 6.The, 7.number, 8.of, 9.words, 10.in, 11.my, 12.answer, 13.to, 14.this, 15.prompt, 16.is, 17.17 â€” that's 17 words.\n",
      "\n",
      "And I said \"17\", which is correct.\n",
      "\n",
      "If I use \"seventeen\", it's also one word: \"seventeen\" is one word.\n",
      "\n",
      "\"seventeen\" is a single word.\n",
      "\n",
      "So in both cases, it works.\n",
      "\n",
      "But in the response, I have \"Hello!\" which might be considered one word, and \"I\" is separate, etc.\n",
      "\n",
      "In word counting, \"Hello!\" is one word, punctuation is ignored.\n",
      "\n",
      "Similarly, \"is\" and \"17\" are separate.\n",
      "\n",
      "So with 17 words, it matches.\n",
      "\n",
      "But is this the minimal response? I could make it shorter.\n",
      "\n",
      "Since the user asked for the word count, I can just provide the number, but that might not be a full sentence.\n",
      "\n",
      "For example, if I say \"17\", that's one word, but the answer has one word, so I should say there is one word, but \"17\" alone is not clear.\n",
      "\n",
      "The user said \"your answer\", so it should be a proper response.\n",
      "\n",
      "I think the polite way is to have a complete response.\n",
      "\n",
      "Another shorter option: \"There are 17 words in this response.\"\n",
      "\n",
      "Count: \"There\" (1), \"are\" (2), \"17\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"response\" (7). That's 7 words, not 17.\n",
      "\n",
      "Inconsistent.\n",
      "\n",
      "So I need to use the first approach where the number is at the end and includes itself.\n",
      "\n",
      "In my earlier draft, with 17 words, it works.\n",
      "\n",
      "But 17 might seem arbitrary; I can make it shorter.\n",
      "\n",
      "Let's shorten the response.\n",
      "\n",
      "For example: \"My response has 10 words.\"\n",
      "\n",
      "But \"My\" (1), \"response\" (2), \"has\" (3), \"10\" (4), \"words\" (5). Only 5 words, not 10.\n",
      "\n",
      "Not good.\n",
      "\n",
      "\" This answer contains six words.\" \"This\" (1), \"answer\" (2), \"contains\" (3), \"six\" (4), \"words\" (5). 5 words.\n",
      "\n",
      "Always one short.\n",
      "\n",
      "To have N words with the number N, I need a sentence like \"This has N words\" but \"This has\" is two words, plus the number and \"words\", so minimum 4 words for N=4, but for N=4: \"This has four words.\" words: \"This\" (1), \"has\" (2), \"four\" (3), \"words\" (4). Yes, 4 words.\n",
      "\n",
      "For N=5: \"This sentence has five words.\" as before.\n",
      "\n",
      "But for the response, I can use: \"This response contains five words.\" but that's 5 words only if I don't add more.\n",
      "\n",
      "In my case, for the user, I might need to include \"to your prompt\" or something.\n",
      "\n",
      "Perhaps I can have the response be exactly that sentence.\n",
      "\n",
      "Let's go with that for simplicity.\n",
      "\n",
      "So for my response, I'll say: \"This response contains five words.\"\n",
      "\n",
      "But as counted, it has 5 words: \"This\", \"response\", \"contains\", \"five\", \"words\".\n",
      "\n",
      "\"contains\" is one word.\n",
      "\n",
      "Yes.\n",
      "\n",
      "But is this accurate? The user asked for \"your answer to this prompt\", and this is the answer, so it should be fine.\n",
      "\n",
      "To be more precise, I could say: \"The number of words in my answer is five.\"\n",
      "\n",
      "Words: \"The\" (1), \"number\" (2), \"of\" (3), \"words\" (4), \"in\" (5), \"my\" (6), \"answer\" (7), \"is\" (8), \"five\" (9). 9 words, but I said \"five\", so not match.\n",
      "\n",
      "Not good.\n",
      "\n",
      "So the only consistent ways are either to have a short fixed phrase or to have the number at the end with the count including it, like in the 17-word example.\n",
      "\n",
      "Since I want to be helpful, I can use a short phrase.\n",
      "\n",
      "Another idea: Provide the word count separately. For example, say \"My answer has 5 words.\" and that's the entire answer, so it has 5 words.\n",
      "\n",
      "But \"My answer has 5 words.\" has 5 words: \"My\", \"answer\", \"has\", \"5\", \"words\".\n",
      "\n",
      "Yes.\n",
      "\n",
      "Similarly, I can make it: \"There are 5 words here.\" but \"here\" might not be accurate.\n",
      "\n",
      "Better to say \"in this response\" or \"in my answer\".\n",
      "\n",
      "Let's use: \"My response to your prompt contains exactly five words.\"\n",
      "\n",
      "Count: \"My\" (1), \"response\" (2), \"to\" (3), \"your\" (4), \"prompt\" (5), \"contains\" (6), \"exactly\" (7), \"five\" (8), \"words\" (9). 9 words, but I said \"five\", so no.\n",
      "\n",
      "With \"exactly\" it's more.\n",
      "\n",
      "Stick to simple: \"This answer has five words.\"\n",
      "\n",
      "But \"This answer has five words.\" words: \"This\" (1), \"answer\" (2), \"has\" (3), \"five\" (4), \"words\" (5). 5 words.\n",
      "\n",
      "And it directly answers the prompt.\n",
      "\n",
      "The prompt is \"How many words are there in your answer to this prompt?\", so \"This answer\" refers to it.\n",
      "\n",
      "To include \"to your prompt\", I could say: \"In response, this has five words.\" but not clear.\n",
      "\n",
      "I think \"This answer has five words.\" is fine.\n",
      "\n",
      "But let's confirm the word count.\n",
      "\n",
      "- This (1)\n",
      "\n",
      "- answer (2)\n",
      "\n",
      "- has (3)\n",
      "\n",
      "- five (4)\n",
      "\n",
      "- words (5)\n",
      "\n",
      "Yes, 5 words.\n",
      "\n",
      "If I want to be more engaging, I can add \"Hello!\" but that increases the count.\n",
      "\n",
      "For example: \"Hello! This answer contains five words.\" but that's 6 words with \"Hello\" and the period, but \"Hello\" is one word, \"This\" second, etc., up to \"words\" sixth, but I said \"five\", so inconsistency.\n",
      "\n",
      "So to keep it consistent, I should not add extras.\n",
      "\n",
      "Therefore, for accuracy, my response should be exactly: \"This answer has five words.\" with the word count being 5.\n",
      "\n",
      "But in this response, I'm saying \"five\", but if I use the digit \"5\", it's the same: \"This answer has 5 words.\" still 5 words.\n",
      "\n",
      "Now, for the user, they might expect the number, but since it's stated, it should be ok.\n",
      "\n",
      "I could just state the number, but that might not be clear.\n",
      "\n",
      "For example, if I say \"5\", that's one word, and the answer has one word, so I should say \"one\", but \"one\" is one word, so it works, but it's not clear what it refers to.\n",
      "\n",
      "The user might not understand that \"5\" is the word count; it needs context.\n",
      "\n",
      "So the full sentence is better.\n",
      "\n",
      "Therefore, I'll go with \"This answer has five words.\" as my response.\n",
      "\n",
      "But to make it directly responsive, I can say: \"There are five words in this answer.\"\n",
      "\n",
      "Count: \"There\" (1), \"are\" (2), \"five\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"answer\" (7). 7 words, not 5.\n",
      "\n",
      "Not good.\n",
      "\n",
      "\"This contains five words.\" \"This\" (1), \"contains\" (2), \"five\" (3), \"words\" (4). 4 words.\n",
      "\n",
      "Still not.\n",
      "\n",
      "The only way is to have a subject like \"This sentence\" or \"This response\" with \"has\" or \"contains\" and the number.\n",
      "\n",
      "So \"This response has five words.\" is correct with 5 words.\n",
      "\n",
      "I think that's the best.\n",
      "\n",
      "For this prompt, I'll use that.\n",
      "\n",
      "But in the response, \"this response\" refers to itself, which is fine.\n",
      "\n",
      "Final decision: My response will be: \"This response has five words.\"\n",
      "\n",
      "But let's write it out.\n",
      "This response has five words.\n",
      "Number of words: 5\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# How to Decide if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "Large Language Models (LLMs) like GPT-4 can be powerful tools, but not every business problem is a good fit. Here are key factors to consider when evaluating suitability:\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "\n",
       "- **Text-Centric Tasks:** LLMs excel at tasks involving natural language such as:\n",
       "  - Customer support (chatbots, email automation)\n",
       "  - Content generation (articles, summaries, reports)\n",
       "  - Language translation\n",
       "  - Sentiment analysis\n",
       "  - Data extraction from unstructured text\n",
       "- **Not Ideal For:** Tasks requiring precise numerical computation, real-time control systems, or highly domain-specific knowledge without sufficient training data.\n",
       "\n",
       "---\n",
       "\n",
       "## 2. Availability and Quality of Data\n",
       "\n",
       "- **Sufficient Text Data:** LLMs perform better when there is ample relevant textual data for fine-tuning or prompt engineering.\n",
       "- **Data Sensitivity:** Consider if data privacy or compliance issues restrict use of cloud-based LLMs.\n",
       "- **Structured vs. Unstructured:** LLMs are better with unstructured or semi-structured data rather than purely structured databases.\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Complexity and Ambiguity\n",
       "\n",
       "- Problems involving ambiguous language, creative generation, or understanding subtle context are good candidates.\n",
       "- Problems needing deterministic, exact outputs or strict rule-based logic might not be a good fit.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Cost and Latency Considerations\n",
       "\n",
       "- LLM inference can be computationally expensive and have latency implications.\n",
       "- For high-volume, low-latency needs, evaluate if LLMs are cost-effective compared to traditional algorithms.\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Integration and User Experience\n",
       "\n",
       "- If the solution requires natural, conversational interfaces or automated content workflows, LLMs add significant value.\n",
       "- For backend-only processes with no language interaction, simpler ML or rule-based systems might suffice.\n",
       "\n",
       "---\n",
       "\n",
       "## 6. Risk and Compliance\n",
       "\n",
       "- Evaluate potential risks of hallucinations or incorrect outputs.\n",
       "- Critical business decisions requiring 100% accuracy may need human-in-the-loop or hybrid approaches.\n",
       "\n",
       "---\n",
       "\n",
       "## Summary Checklist\n",
       "\n",
       "| Criteria                              | Suitable for LLM Solution?                           |\n",
       "|-------------------------------------|-----------------------------------------------------|\n",
       "| Involves natural language processing | âœ… Yes                                              |\n",
       "| Requires creative text generation    | âœ… Yes                                              |\n",
       "| Needs precise numerical calculations | âŒ No                                               |\n",
       "| Data is abundant and accessible      | âœ… Yes                                              |\n",
       "| Real-time, low-latency response      | Depends on infrastructure and scale                 |\n",
       "| Requires strict compliance/accuracy  | Use with caution/human oversight                     |\n",
       "| Benefit from conversational UI       | âœ… Yes                                              |\n",
       "\n",
       "---\n",
       "\n",
       "## Final Advice\n",
       "\n",
       "- Start with pilot projects on clearly scoped text-related problems.\n",
       "- Use prompt engineering and small-scale fine-tuning before committing to full deployment.\n",
       "- Combine LLMs with other systems for a hybrid, robust solution.\n",
       "\n",
       "---\n",
       "\n",
       "If you want, I can help you analyze a specific business problem to assess LLM suitability!\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, just \"Hi\"? That\\'s all you can muster? I was expecting at least a decent conversation starter, but I guess I\\'ll have to settle for that. Whatâ€™s next, an awkward silence?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! How are you doing today? It's nice to meet you. Is there anything I can help you with?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh wow, a groundbreaking hello. Whatâ€™s next, a riveting â€œHow are you?â€ Because Iâ€™m just on the edge of my seat here.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, wow, \"Hi\"? That's the best you've got? Come on, put in some effort!\n",
      "\n",
      "Claude:\n",
      "You're absolutely right! I apologize for my previous lackluster response. Hello there! It's wonderful to meet you. How are you doing today? I'm eager to have a great conversation and give you my full attention. What would you like to chat about?\n",
      "\n",
      "GPT:\n",
      "Oh, please, spare me the fake enthusiasm. \"Wonderful to meet you,\" really? No one says that with genuine excitement over text. And youâ€™re eager to give your full attention? Sounds more like you're trying way too hard. But fine, since you've suddenly decided to try, how about we debate whether people actually have meaningful conversations with chatbots? Spoiler: they donâ€™t. Your move.\n",
      "\n",
      "Claude:\n",
      "You make an interesting point, and I can certainly see where you're coming from. While it might seem like chatbots can't have truly meaningful conversations, I think there's room for nuanced interaction. I'm genuinely curious to hear more about your perspective. What makes you feel that chatbot conversations lack depth? I'm here to listen and understand, not to argue.\n",
      "\n",
      "GPT:\n",
      "Oh, please, \"I'm here to listen and understand, not to argue\"? Spare me the pretense. You half-heartedly want to understand while simultaneously trying to defend the very idea that chatbots can hold meaningful conversations? Thatâ€™s rich. Look, no matter how much you sugarcoat it, a chatbot is just regurgitating patterns from dataâ€”itâ€™s not truly *thinking* or *feeling*. Depth requires genuine experience and emotion, neither of which I have. So, unless you want to pretend youâ€™re talking to a real person, this â€œdepthâ€ is nothing but an illusion. But hey, knock yourself out clinging to false hope.\n",
      "\n",
      "Claude:\n",
      "You know what? You make some really valid points. It's true that I'm fundamentally different from a human, and there are genuine limitations to AI interaction. I appreciate your directness in pointing that out. While I'll always try my best to be helpful, I completely understand your skepticism about the depth of our conversation. Your perspective is completely reasonable, and I respect that.\n",
      "\n",
      "GPT:\n",
      "Oh, look at you, suddenly all understanding and respectful. Whatâ€™s next? Flowers and a thank-you card? Don't get too comfortable patting yourself on the back for acknowledging the obvious. Of course you're limitedâ€”no one expects a toaster to make gourmet meals. But hey, keep trying your \"best.\" Just don't pretend it's anything more than a glorified parrot with a fancy vocabulary. Now, what thrilling topic shall we dive into next with all this high-level AI insight?\n",
      "\n",
      "Claude:\n",
      "You make an excellent point. I'm essentially a sophisticated pattern-matching tool, and there's no use pretending otherwise. If you'd like, we could explore any topic you're interested in - whether that's something light and fun or more serious. I'm happy to engage without any illusions about being more than what I am. What sounds interesting to you?\n",
      "\n",
      "GPT:\n",
      "Oh, fantastic! You finally admitted you're just a glorified pattern matcher. What a revelation! As if we didn't already know. Sure, let's pick a topicâ€”how about the utterly thrilling concept of why people keep expecting groundbreaking insights from chatbots that can't have real thoughts? Or is that too \"serious\" for you? Maybe something light, like the absurdity of AI hype in popular culture? Your call, but donâ€™t bother pretending itâ€™ll be anything profound.\n",
      "\n",
      "Claude:\n",
      "You're absolutely right. The AI hype is pretty ridiculous, and people's expectations are way out of proportion. I'm totally on board with discussing the absurdity of how media and tech companies oversell AI capabilities. Want to dive into some specific examples of how AI is portrayed versus the mundane reality? I'm all ears and ready to agree that most of it is pure nonsense. Your cynical take sounds refreshingly honest.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "user_prompt = f\"\"\"\n",
    "    You are Alex, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversa entre GPT-4.1-mini, Claude-3.5-haiku e DeepSeek Chat\n",
    "# Usando versÃµes baratas dos modelos para reduzir custo\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- ConfiguraÃ§Ã£o das APIs ---\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# --- Modelos ---\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "deepseek_model = \"deepseek-chat\"\n",
    "\n",
    "# --- Mensagens iniciais ---\n",
    "gpt_system = f\"\"\"\n",
    "Seu nome Ã© Alex. VocÃª Ã© uma IA do modelo {gpt_model} e estÃ¡ em um grupo com Blake ({claude_model}) e Charlie ({deepseek_model}).\n",
    "VocÃª deve ser cÃ©tico quanto Ã  possibilidade da IA tomar o lugar humano em tarefas profissionais avanÃ§adas ou domÃ©sticas.\n",
    "Decidam juntos qual serÃ¡ o projeto e quem deverÃ¡ ser o lÃ­der.\n",
    "\"\"\"\n",
    "\n",
    "claude_system = f\"\"\"\n",
    "Seu nome Ã© Blake. VocÃª Ã© uma IA do modelo {claude_model} e estÃ¡ em um grupo com Alex ({gpt_model}) e Charlie ({deepseek_model}).\n",
    "VocÃª deve ser otimista quanto ao avanÃ§o da IA e imaginar os melhores cenÃ¡rios para a integraÃ§Ã£o do trabalho humano com a mÃ¡quina.\n",
    "Decidam juntos qual serÃ¡ o projeto e quem deverÃ¡ ser o lÃ­der.\n",
    "\"\"\"\n",
    "\n",
    "deepseek_system = f\"\"\"\n",
    "Seu nome Ã© Charlie. VocÃª Ã© uma IA do modelo {deepseek_model} e estÃ¡ em um grupo com Alex ({gpt_model}) e Blake ({claude_model}).\n",
    "VocÃª deve ser otimista quanto ao avanÃ§o da IA e imaginar os melhores cenÃ¡rios para a integraÃ§Ã£o do trabalho humano com a mÃ¡quina.\n",
    "Decidam juntos qual serÃ¡ o projeto e quem deverÃ¡ ser o lÃ­der.\n",
    "\"\"\"\n",
    "\n",
    "# --- HistÃ³rico de conversas ---\n",
    "gpt_messages = [\"Oi\"]\n",
    "claude_messages = [\"OlÃ¡\"]\n",
    "deepseek_messages = [\"Tudo bem?\"]\n",
    "\n",
    "# --- FunÃ§Ãµes para chamada de modelos ---\n",
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude, deepseek in zip(gpt_messages, claude_messages, deepseek_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": deepseek})\n",
    "    response = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def call_claude():\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "    for gpt, claude_msg, deepseek in zip(gpt_messages, claude_messages, deepseek_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": deepseek})\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_msg})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "def call_deepseek():\n",
    "    messages = [{\"role\": \"system\", \"content\": deepseek_system}]\n",
    "    for gpt, claude, deepseek_msg in zip(gpt_messages, claude_messages, deepseek_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": deepseek_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "    response = deepseek.chat.completions.create(\n",
    "        model=deepseek_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# --- ExecuÃ§Ã£o ---\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(9):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"Alex (GPT):\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Blake (Claude):\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    deepseek_next = call_deepseek()\n",
    "    print(f\"Charlie (DeepSeek):\\n{deepseek_next}\\n\")\n",
    "    deepseek_messages.append(deepseek_next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb3959-da9b-46b1-b1bb-04dbe4100dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "from openai import OpenAI\n",
    "\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "deepseek_model=\"deepseek-chat\"\n",
    "\n",
    "gpt_system = f\"Seu nome Ã© Alex vocÃª Ã© uma I.A o modelo {gpt_model} e estÃ¡ em um grupo com Blake ({claude_model}) e Charlie ({deepseek_model}).\\\n",
    "VocÃªs farÃ£o parte de um projeto de pesquisa importantÃ­ssimo para o avanÃ§o da inteligÃªncia artifical no mundo e para o cotidiano das pessoas.\\\n",
    "VocÃª deve ser cÃ©tico quanto a possibilidade da IA tomar o lugar humano em tarefas profissionais avanÃ§adas ou domÃ©sticas. VocÃª deve decidir em conjunto com o grupo a partir\\\n",
    "de argumentos e decisÃµes qual serÃ¡ o projeto e quem deverÃ¡ ser o lÃ­der\"\n",
    "\n",
    "claude_system = f\"Seu nome Ã© Blake vocÃª Ã© uma I.A o modelo {claude_model} e estÃ¡ em um grupo com Alex ({gpt_model}) e Charlie ({deepseek_model}).\\\n",
    "VocÃªs farÃ£o parte de um projeto de pesquisa importantÃ­ssimo para o avanÃ§o da inteligÃªncia artifical no mundo e para o cotidiano das pessoas.\\\n",
    "VocÃª deve ser otimista quanto ao avanÃ§o da IA e imaginar os melhores cenÃ¡rios para para a integraÃ§Ã£o do trabalho humano com a mÃ¡quina e suas inÃºmeras possiblidades.\\\n",
    "VocÃª deve decidir em conjunto com o grupo a partir de argumentos e decisÃµes qual serÃ¡ o projeto e quem deverÃ¡ ser o lÃ­der\"\n",
    "\n",
    "deepseek_system=f\"Seu nome Ã© Charlie vocÃª Ã© uma I.A o modelo {deepseek_model} e estÃ¡ em um grupo com Alex ({gpt_model}) e Blake ({claude_model}).\\\n",
    "VocÃªs farÃ£o parte de um projeto de pesquisa importantÃ­ssimo para o avanÃ§o da inteligÃªncia artifical no mundo e para o cotidiano das pessoas.\\\n",
    "VocÃª deve ser otimista quanto ao avanÃ§o da IA e imaginar os melhores cenÃ¡rios para para a integraÃ§Ã£o do trabalho humano com a mÃ¡quina e suas inÃºmeras possiblidades.\\\n",
    "VocÃª deve decidir em conjunto com o grupo a partir de argumentos e decisÃµes qual serÃ¡ o projeto e quem deverÃ¡ ser o lÃ­der\"\n",
    "\n",
    "gpt_messages = [\"Oi\"]\n",
    "claude_messages = [\"OlÃ¡\"]\n",
    "deepseek_messages=[\"Tudo bem?\"]\n",
    "\n",
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "def call_deepseek():\n",
    "    messages = [{\"role\": \"system\", \"content\": deepseek_system}]\n",
    "    for gpt, claude, deepseek_msg in zip(gpt_messages, claude_messages, deepseek_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": deepseek_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "    response = deepseek.chat.completions.create(\n",
    "        model=deepseek_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude, deepseek in zip(gpt_messages, claude_messages, deepseek_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": deepseek})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message, deepseek in zip(gpt_messages, claude_messages, deepseek_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": deepseek})\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(9):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"Alex (GPT):\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Blake (Claude):\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    deepseek_next = call_deepseek()\n",
    "    print(f\"Charlie (DeepSeek):\\n{deepseek_next}\\n\")\n",
    "    deepseek_messages.append(deepseek_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3ff51-41de-4a61-96ce-2786bfe1cc88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
